{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5845954",
   "metadata": {},
   "source": [
    "# BigQuery Connection Setup for Belgian Brewery Project\n",
    "\n",
    "This notebook establishes and tests the connection to Google BigQuery for the Belgian Brewery \"Glide Template\" Strategy Project.\n",
    "\n",
    "## Prerequisites\n",
    "- Google Cloud Project with BigQuery API enabled\n",
    "- Service account key or authentication setup\n",
    "- Required Python packages installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conda environment for this project\n",
    "# conda create -n be-brew-py311 python=3.11\n",
    "# conda activate be-brew-py311\n",
    "\n",
    "# Install required packages using the requirements.txt file\n",
    "# Install required packages (run this cell first if packages are not installed)\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e51c6a",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13862706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for BigQuery connection\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789f822",
   "metadata": {},
   "source": [
    "## Authentication Setup\n",
    "\n",
    "Choose one of the following authentication methods:\n",
    "1. Service Account Key File\n",
    "2. Application Default Credentials (if running on Google Cloud)\n",
    "3. Manual credentials setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using Service Account Key File\n",
    "# Uncomment and modify the path to your service account key\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/your/service-account-key.json'\n",
    "\n",
    "# Option 2: Using explicit credentials (replace with your actual service account key content)\n",
    "# service_account_info = {\n",
    "#     \"type\": \"service_account\",\n",
    "#     \"project_id\": \"your-project-id\",\n",
    "#     \"private_key_id\": \"your-private-key-id\",\n",
    "#     # ... other fields from your service account key\n",
    "# }\n",
    "# credentials = service_account.Credentials.from_service_account_info(service_account_info)\n",
    "\n",
    "# For now, we'll use application default credentials\n",
    "# Make sure you've run: gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12de428",
   "metadata": {},
   "source": [
    "## BigQuery Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5777998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Google Cloud Project ID\n",
    "PROJECT_ID = \"your-project-id\"  # Replace with your actual project ID\n",
    "DATASET_ID = \"belgian_brewery\"   # Dataset for our brewery project\n",
    "\n",
    "# Initialize BigQuery client\n",
    "try:\n",
    "    # Option 1: Using default credentials\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    \n",
    "    # Option 2: Using explicit credentials (uncomment if using service account)\n",
    "    # client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
    "    \n",
    "    print(f\"Successfully connected to BigQuery project: {PROJECT_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to BigQuery: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c9241",
   "metadata": {},
   "source": [
    "## Test Connection with Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the connection with a simple query\n",
    "test_query = \"\"\"\n",
    "SELECT \n",
    "    'BigQuery connection successful!' as status,\n",
    "    CURRENT_DATETIME() as timestamp,\n",
    "    @@project_id as project_id\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Execute the test query\n",
    "    query_job = client.query(test_query)\n",
    "    results = query_job.result()\n",
    "    \n",
    "    # Convert to DataFrame for better display\n",
    "    df_test = results.to_dataframe()\n",
    "    print(\"Connection Test Results:\")\n",
    "    print(df_test)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error executing test query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77803b34",
   "metadata": {},
   "source": [
    "## Create Dataset for Belgian Brewery Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for the Belgian brewery project\n",
    "dataset_id = f\"{PROJECT_ID}.{DATASET_ID}\"\n",
    "\n",
    "try:\n",
    "    # Check if dataset already exists\n",
    "    dataset = client.get_dataset(dataset_id)\n",
    "    print(f\"Dataset {dataset_id} already exists.\")\n",
    "except:\n",
    "    # Create the dataset if it doesn't exist\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = \"US\"  # or \"EU\" depending on your preference\n",
    "    dataset.description = \"Dataset for Belgian Brewery Glide Template Strategy Project\"\n",
    "    \n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print(f\"Created dataset {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a518c6",
   "metadata": {},
   "source": [
    "## Prepare for Data Upload\n",
    "\n",
    "Next steps will involve:\n",
    "1. Loading the Belgian beers and breweries data from Google Sheets\n",
    "2. Uploading raw data to BigQuery tables\n",
    "3. Setting up dbt for data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload DataFrame to BigQuery\n",
    "def upload_dataframe_to_bigquery(df, table_name, dataset_id=DATASET_ID, if_exists='replace'):\n",
    "    \"\"\"\n",
    "    Upload a pandas DataFrame to BigQuery\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame to upload\n",
    "        table_name: name of the BigQuery table\n",
    "        dataset_id: BigQuery dataset ID\n",
    "        if_exists: what to do if table exists ('replace', 'append', 'fail')\n",
    "    \"\"\"\n",
    "    table_id = f\"{PROJECT_ID}.{dataset_id}.{table_name}\"\n",
    "    \n",
    "    # Configure the job\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\" if if_exists == 'replace' else \"WRITE_APPEND\",\n",
    "        autodetect=True  # Automatically detect schema\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Upload the DataFrame\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()  # Wait for the job to complete\n",
    "        \n",
    "        print(f\"Successfully uploaded {len(df)} rows to {table_id}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading data to {table_id}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test function with sample data\n",
    "sample_data = pd.DataFrame({\n",
    "    'test_column': ['value1', 'value2', 'value3'],\n",
    "    'timestamp': pd.Timestamp.now()\n",
    "})\n",
    "\n",
    "print(\"Testing upload function with sample data:\")\n",
    "upload_dataframe_to_bigquery(sample_data, 'connection_test', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb8ab5",
   "metadata": {},
   "source": [
    "## Verify Sample Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the test table to verify upload worked\n",
    "verify_query = f\"\"\"\n",
    "SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.connection_test`\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    query_job = client.query(verify_query)\n",
    "    results = query_job.result()\n",
    "    df_verify = results.to_dataframe()\n",
    "    \n",
    "    print(\"Sample data successfully uploaded and retrieved:\")\n",
    "    print(df_verify)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error querying test table: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d8ffe",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "✅ BigQuery connection established  \n",
    "✅ Dataset created  \n",
    "✅ Upload function tested  \n",
    "\n",
    "**Ready for the main project workflow:**\n",
    "\n",
    "1. **Data Ingestion**: Load Belgian brewery data from Google Sheets\n",
    "2. **Data Enrichment**: Use Python + geocoding API to get brewery locations  \n",
    "3. **dbt Setup**: Create transformation pipeline\n",
    "4. **Hex Dashboard**: Build analytics dashboard\n",
    "\n",
    "Update the `PROJECT_ID` variable above with your actual Google Cloud Project ID to proceed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
